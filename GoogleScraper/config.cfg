; Configuration parameters that control the scraping process. You will most
; likely want to change these values.
[SCRAPING]
; The search queries to search for, separated by newlines. Intend every new
; keyword-line at least more than the next keyword.
keywords:  Apples
           Peaches
           Bananas
           Cheapest Hotel in California
           inurl: "admin.php"

; The keyword file. If this is a valid file path, the keywords params will be ignored and
; the ones from the file will be taken. Each keyword must be on a separate line.
keyword_file:

; How many results per SERP page
num_results_per_page: 10

; How many pages should be requested for each single keyword
num_pages_for_keyword: 1

; The search offset on which page to start scraping.
; Pages begin at 1
search_offset: 1

; The proxy policy. Not implemented yet.
proxy_policy:

; List of supported search engines
; If you add support for another search engine (of course implement it in the
; appropriate places) and add it in this list.
supported_search_engines: google, yandex, bing, yahoo, baidu, duckduckgo

; The search engine to use. Currently there are the following supported search engines:
; - google
; - bing
; - yandex
; - baidu
; - duckduckgo
search_engine: yandex

;;; The base search urls 
;;; Ready to append the parameters at the end to fine tune the search.

; The google base search url
google_search_url: https://www.google.com/#

; The yandex base search url
yandex_search_url: http://yandex.ru/yandsearch?

; The bing base search url
bing_search_url: http://www.bing.com/search?

; The yahoo base search url
yahoo_search_url: https://de.search.yahoo.com/search?

; The baidu base search url
baidu_search_url: http://www.baidu.com/s?

; The duckduckgo base search url
duckduckgo_search_url: https://duckduckgo.com/

; The search type. Currently, the following search modes are
; supported for some search engine: normal, video, news and image search.
; "normal" search type is supported in all search engines.
search_type: normal

; The scrape method. Can be 'http' or  'sel' or 'http_async'
; http mode uses http packets directly, whereas sel mode uses a real browser (or phantomsjs).
; http_async uses grequests (requests on top of gevent) and probably epoll on unix systems.
scrapemethod: sel

; If scraping with the own IP address should be allowed.
; If this is set to False and you don't specify any proxies,
; GoogleScraper cannot run.
use_own_ip: True

; Deep scrape. Not implemented yet.
deep_scrape: False

; Global configuration parameters that apply on all modes.
[GLOBAL]
; The proxy file. If this is a valid file path, each line will represent a proxy.
; Example file:
;        socks5 23.212.45.13:1080 username:password
;        socks4 23.212.45.13:80 username:password
;        http 23.212.45.13:80
proxy_file:

; Proxies stored in a MySQL database. If you set a parameter here, GoogleScraper will look for proxies
; in a table named 'proxies' for proxies with the following format:
;
; CREATE TABLE proxies (
;   id INTEGER PRIMARY KEY NOT NULL,
;   host VARCHAR(255) NOT NULL,
;   port SMALLINT,
;   username VARCHAR(255),
;   password VARCHAR(255),
;   protocol ENUM('socks5', 'socks4', 'http')
; );
;
; Specify the connection details in the following format: mysql://<username>:<password>@<host>/<dbname>
; Example: mysql://root:soemshittypass@localhost/supercoolproxies
mysql_proxy_db:

; Set the debug level of the application. Must be an integer.
; CRITICAL = 50
; FATAL = CRITICAL
; ERROR = 40
; WARNING = 30
; WARN = WARNING
; INFO = 20
; DEBUG = 10
; NOTSET = 0
debug: INFO

; If set, prints all found results to stdout. You shouldn't set
; this flag, because it shadows a lot of debug information.
; Results are stored in sqlite3 databases anyway.
print: False

; The verbosity level
verbosity: 1

; The basic search url
base_search_url: http://www.google.com/search

; Whether caching shall be enabled
do_caching: False

; If set, then compress/decompress cached files
compress_cached_files: True

; The relative path to the cache directory
cachedir: .scrapecache/

; After how many hours should the cache be cleaned
clean_cache_after: 24

; Commit changes to the database every nth GET/POST request
commit_interval: 15

; Check one-to-one relationship of cached file. For Devs only.
check_oto: False

; If the search should be simulated instead of being done.
; Useful to learn about the quantity of keywords to scrape and such.
; Won't fire any requesets.
simulate: False

; The database name, with a timestamp as fmt
; All results will be saved to a directory results
database_name: google_scraper.db

; Internal use only
fix_cache_names: False

; All settings that only apply for requesting with real browsers.
[SELENIUM]
; The maximal amount of selenium browser windows running in parallel
num_browser_instances: 6

; The base Google URL for SelScraper objects
sel_scraper_base_url: http://www.google.com/

; which browser to use in selenium mode. Valid values: ('Chrome', 'Firefox', 'Phantomjs')
sel_browser: chrome

; Manual captcha solving
; If this parameter is set to a Integer, the browser waits for the user
; to enter the captcha manually whenever Google detected the script as malicious.
;
; Set to False to disable.
; If the captcha isn't solved in the specified time interval, the browser instance
; with the current proxy is discarded.
manual_captcha_solving: True

; Sleeping ranges.
; The scraper in selenium mode makes random modes every N seconds as specified in the given intervals.
; Format: [Every Nth second when to sleep]; ([Start range], [End range])
sleeping_ranges: 7; 1, 5
                 57; 50, 120
                 127; 180, 360

; All settings that target the raw http packet scraping mode.
[HTTP]
; The number of concurrent threads that are used for scraping
num_threads: 1

[HTTP_ASYNC]
; The number of concurrent threads that are used for scraping
num_threads: 1
